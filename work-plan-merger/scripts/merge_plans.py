#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å·¥ä½œè§„åˆ’æ•´åˆå·¥å…·
å°†å¤šä¸ªå­è§„åˆ’æ™ºèƒ½åŒ¹é…å¹¶æ•´åˆåˆ°æ€»çº²è§„åˆ’çš„ç›¸åº”éƒ¨åˆ†
"""

import argparse
import re
import json
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from difflib import SequenceMatcher
import jieba


class PlanMerger:
    def __init__(self):
        self.heading_pattern = re.compile(r'^(#{1,6})\s+(.+)$', re.MULTILINE)
        self.load_default_matching_rules()

    def load_default_matching_rules(self):
        """åŠ è½½é»˜è®¤åŒ¹é…è§„åˆ™"""
        self.matching_rules = {
            'keywords_weight': 0.5,
            'structure_weight': 0.3,
            'position_weight': 0.2,
            'min_similarity': 0.1,
            'content_separators': {
                'main': '\n--- å­è§„åˆ’å†…å®¹ ---\n',
                'sub': '\n\n'
            }
        }

    def merge_plans(self, master_file: str, subplans_dir: str, output_file: str) -> Dict:
        """æ•´åˆè§„åˆ’æ–‡æ¡£"""
        # è¯»å–æ€»çº²æ–‡æ¡£
        with open(master_file, 'r', encoding='utf-8') as f:
            master_content = f.read()

        # åˆ†ææ€»çº²ç»“æ„
        master_structure = self._analyze_document_structure(master_content)

        # è¯»å–æ‰€æœ‰å­è§„åˆ’
        subplans = self._load_subplans(subplans_dir)

        # åŒ¹é…å­è§„åˆ’åˆ°æ€»çº²ç« èŠ‚
        matches = self._match_subplans_to_sections(subplans, master_structure)

        # ç”Ÿæˆæ•´åˆæ–‡æ¡£
        merged_content = self._generate_merged_document(
            master_content, master_structure, matches, subplans
        )

        # ä¿å­˜ç»“æœ
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(merged_content)

        # ç”Ÿæˆæ•´åˆæŠ¥å‘Š
        report = {
            'master_file': master_file,
            'subplans_count': len(subplans),
            'matches_found': len(matches),
            'output_file': output_file,
            'matching_details': matches,
            'unmatched_subplans': [sp for sp in subplans if sp['name'] not in [m['subplan'] for m in matches]]
        }

        return report

    def _analyze_document_structure(self, content: str) -> List[Dict]:
        """åˆ†ææ–‡æ¡£ç»“æ„"""
        headings = []
        lines = content.split('\n')

        for i, line in enumerate(lines):
            match = self.heading_pattern.match(line)
            if match:
                level = len(match.group(1))
                title = match.group(2).strip()

                # æå–ç« èŠ‚å†…å®¹
                section_content = self._extract_section_content(content, i, level)

                headings.append({
                    'level': level,
                    'title': title,
                    'line_number': i + 1,
                    'content': section_content,
                    'keywords': self._extract_keywords(title + ' ' + section_content[:200])
                })

        return headings

    def _extract_section_content(self, content: str, start_line: int, current_level: int) -> str:
        """æå–ç« èŠ‚å†…å®¹"""
        lines = content.split('\n')
        content_lines = []

        # è·³è¿‡å½“å‰æ ‡é¢˜è¡Œï¼Œä»ä¸‹ä¸€è¡Œå¼€å§‹
        for i in range(start_line + 1, len(lines)):
            line = lines[i]

            # æ£€æŸ¥æ˜¯å¦é‡åˆ°åŒçº§æˆ–æ›´é«˜çº§æ ‡é¢˜
            heading_match = self.heading_pattern.match(line)
            if heading_match:
                heading_level = len(heading_match.group(1))
                if heading_level <= current_level:
                    break

            content_lines.append(line)

        return '\n'.join(content_lines).strip()

    def _extract_keywords(self, text: str) -> List[str]:
        """æå–å…³é”®è¯"""
        # ä½¿ç”¨jiebaè¿›è¡Œä¸­æ–‡åˆ†è¯
        words = jieba.lcut(text.lower())

        # è¿‡æ»¤åœç”¨è¯å’ŒçŸ­è¯
        stop_words = {'çš„', 'å’Œ', 'åœ¨', 'æ˜¯', 'ä¸º', 'äº†', 'ä¸', 'ä¸­', 'æœ‰', 'åŠ', 'ç­‰', 'æˆ–', 'å°†', 'ä¼š', 'å¯¹', 'è¿›è¡Œ', 'å·¥ä½œ', 'è§„åˆ’', 'è®¡åˆ’'}
        keywords = [word for word in words if len(word) > 1 and word not in stop_words and word.strip()]

        # è¿”å›å‰10ä¸ªæœ€å¸¸è§çš„å…³é”®è¯
        word_freq = {}
        for word in keywords:
            word_freq[word] = word_freq.get(word, 0) + 1

        return [word for word, _ in sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:10]]

    def _load_subplans(self, subplans_dir: str) -> List[Dict]:
        """åŠ è½½æ‰€æœ‰å­è§„åˆ’æ–‡ä»¶"""
        subplans = []
        subplan_path = Path(subplans_dir)

        for file_path in subplan_path.glob('*.md'):
            # è·³è¿‡æ€»çº²æ–‡ä»¶ï¼ˆå‡è®¾æ€»çº²æ–‡ä»¶ä¸åœ¨æ­¤ç›®å½•ä¸­ï¼Œæˆ–è€…é€šè¿‡åç§°æ’é™¤ï¼‰
            if file_path.name.startswith('æ€»çº²') or file_path.name.startswith('master'):
                continue

            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            # åˆ†æå­è§„åˆ’ç»“æ„
            structure = self._analyze_document_structure(content)

            subplans.append({
                'name': file_path.stem,
                'file_path': str(file_path),
                'content': content,
                'structure': structure,
                'keywords': self._extract_keywords(content)
            })

        return subplans

    def _match_subplans_to_sections(self, subplans: List[Dict], master_structure: List[Dict]) -> List[Dict]:
        """å°†å­è§„åˆ’åŒ¹é…åˆ°æ€»çº²ç« èŠ‚"""
        matches = []

        for subplan in subplans:
            best_match = self._find_best_section_match(subplan, master_structure)

            if best_match and best_match['similarity'] >= self.matching_rules['min_similarity']:
                matches.append({
                    'subplan': subplan['name'],
                    'section_index': best_match['section_index'],
                    'section_title': best_match['section_title'],
                    'similarity': best_match['similarity'],
                    'match_reason': best_match['reason']
                })

        return matches

    def _find_best_section_match(self, subplan: Dict, master_structure: List[Dict]) -> Optional[Dict]:
        """ä¸ºå­è§„åˆ’æ‰¾åˆ°æœ€ä½³åŒ¹é…ç« èŠ‚"""
        best_match = None
        max_similarity = 0

        for i, section in enumerate(master_structure):
            # è€ƒè™‘1-3çº§æ ‡é¢˜ä½œä¸ºåŒ¹é…ç›®æ ‡
            if section['level'] > 3:
                continue

            similarity = self._calculate_similarity(subplan, section)

            if similarity > max_similarity:
                max_similarity = similarity
                best_match = {
                    'section_index': i,
                    'section_title': section['title'],
                    'similarity': similarity,
                    'reason': self._generate_match_reason(subplan, section, similarity)
                }

        return best_match if max_similarity > 0 else None

    def _calculate_similarity(self, subplan: Dict, section: Dict) -> float:
        """è®¡ç®—å­è§„åˆ’ä¸ç« èŠ‚çš„ç›¸ä¼¼åº¦"""
        # å…³é”®è¯ç›¸ä¼¼åº¦
        subplan_keywords = set(subplan['keywords'])
        section_keywords = set(section['keywords'])

        if not subplan_keywords or not section_keywords:
            keyword_similarity = 0
        else:
            intersection = subplan_keywords.intersection(section_keywords)
            union = subplan_keywords.union(section_keywords)
            keyword_similarity = len(intersection) / len(union) if union else 0

        # æ–‡æœ¬ç›¸ä¼¼åº¦ï¼ˆä½¿ç”¨ç®€å•çš„å­—ç¬¦ä¸²åŒ¹é…ï¼‰
        text_similarity = SequenceMatcher(
            None,
            subplan['content'][:500],
            section['title'] + ' ' + section['content'][:500]
        ).ratio()

        # ç»¼åˆç›¸ä¼¼åº¦
        total_similarity = (
            keyword_similarity * self.matching_rules['keywords_weight'] +
            text_similarity * (1 - self.matching_rules['keywords_weight'])
        )

        return total_similarity

    def _generate_match_reason(self, subplan: Dict, section: Dict, similarity: float) -> str:
        """ç”ŸæˆåŒ¹é…åŸå› è¯´æ˜"""
        common_keywords = set(subplan['keywords']).intersection(set(section['keywords']))

        reason_parts = []
        if common_keywords:
            reason_parts.append(f"å…±åŒå…³é”®è¯: {', '.join(list(common_keywords)[:3])}")

        if similarity > 0.5:
            reason_parts.append("å†…å®¹é«˜åº¦ç›¸å…³")
        elif similarity > 0.3:
            reason_parts.append("å†…å®¹è¾ƒä¸ºç›¸å…³")
        else:
            reason_parts.append("å†…å®¹éƒ¨åˆ†ç›¸å…³")

        return "; ".join(reason_parts)

    def _generate_merged_document(self, master_content: str, master_structure: List[Dict],
                                matches: List[Dict], subplans: List[Dict]) -> str:
        """ç”Ÿæˆæ•´åˆåçš„æ–‡æ¡£"""
        lines = master_content.split('\n')
        result_lines = []

        # åˆ›å»ºå­è§„åˆ’æŸ¥æ‰¾å­—å…¸
        subplan_dict = {sp['name']: sp for sp in subplans}

        # æŒ‰è¡Œå·æ’åºåŒ¹é…
        matches_by_line = sorted(matches,
                               key=lambda x: master_structure[x['section_index']]['line_number'],
                               reverse=True)

        processed_indices = set()

        for i, line in enumerate(lines):
            result_lines.append(line)

            # æ£€æŸ¥æ˜¯å¦æ˜¯éœ€è¦æ’å…¥å†…å®¹çš„ä½ç½®
            for match in matches_by_line:
                section_idx = match['section_index']
                section = master_structure[section_idx]

                if (i == section['line_number'] and
                    section_idx not in processed_indices):

                    # æ’å…¥å­è§„åˆ’å†…å®¹
                    subplan = subplan_dict[match['subplan']]
                    separator = self.matching_rules['content_separators']['main']

                    result_lines.append(separator)
                    result_lines.append(f"### ğŸ“‹ {subplan['name']}")
                    result_lines.append("")

                    # æ·»åŠ å­è§„åˆ’å†…å®¹
                    subplan_lines = subplan['content'].split('\n')
                    for subline in subplan_lines:
                        if subline.strip():  # è·³è¿‡ç©ºè¡Œ
                            result_lines.append(f"  {subline}")

                    result_lines.append("")
                    processed_indices.add(section_idx)
                    break

        return '\n'.join(result_lines)


def main():
    parser = argparse.ArgumentParser(description='æ•´åˆå·¥ä½œè§„åˆ’æ–‡æ¡£')
    parser.add_argument('master_file', help='æ€»çº²è§„åˆ’æ–‡ä»¶è·¯å¾„')
    parser.add_argument('subplans_dir', help='å­è§„åˆ’æ–‡ä»¶ç›®å½•')
    parser.add_argument('output_file', help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--report', '-r', help='ç”ŸæˆåŒ¹é…æŠ¥å‘Šæ–‡ä»¶')
    parser.add_argument('--verbose', '-v', action='store_true', help='æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯')

    args = parser.parse_args()

    merger = PlanMerger()

    if args.verbose:
        print("ğŸš€ å¼€å§‹æ•´åˆå·¥ä½œè§„åˆ’...")
        print(f"ğŸ“– æ€»çº²æ–‡ä»¶: {args.master_file}")
        print(f"ğŸ“ å­è§„åˆ’ç›®å½•: {args.subplans_dir}")
        print(f"ğŸ’¾ è¾“å‡ºæ–‡ä»¶: {args.output_file}")
        print()

    try:
        report = merger.merge_plans(args.master_file, args.subplans_dir, args.output_file)

        print("âœ… è§„åˆ’æ•´åˆå®Œæˆ!")
        print(f"ğŸ“Š å¤„ç†äº† {report['subplans_count']} ä¸ªå­è§„åˆ’æ–‡ä»¶")
        print(f"ğŸ”— æˆåŠŸåŒ¹é… {report['matches_found']} ä¸ªå­è§„åˆ’åˆ°ç›¸åº”ç« èŠ‚")

        if report['unmatched_subplans']:
            print(f"âš ï¸  æœªåŒ¹é…çš„å­è§„åˆ’: {len(report['unmatched_subplans'])} ä¸ª")
            for unmatched in report['unmatched_subplans']:
                print(f"   - {unmatched['name']}")

        if args.report:
            with open(args.report, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
            print(f"ğŸ“‹ åŒ¹é…æŠ¥å‘Šå·²ä¿å­˜åˆ°: {args.report}")

    except Exception as e:
        print(f"âŒ æ•´åˆè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        return 1

    return 0


if __name__ == "__main__":
    import sys
    sys.exit(main())